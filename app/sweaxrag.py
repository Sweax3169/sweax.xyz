# -*- coding: utf-8 -*-
# sweaxrag.py â€” Hafif RAM sÃ¼rÃ¼mÃ¼ (Render 512 MB dostu)

import os, json, re, importlib
import requests
from urllib.parse import quote
from datetime import datetime
from zoneinfo import ZoneInfo
from bs4 import BeautifulSoup
# ---- Konfig / modÃ¼ler bayraklar ----
SWEAX_LIGHT = os.environ.get("SWEAX_LIGHT", "1") == "1"  # 1: en hafif mod
HEADERS = {"User-Agent": "SweaxAI-lite/1.0 (+educational use)"}

DATA_JSON  = "bilgi_deposu.json"
FAISS_FILE = "bilgi_index.faiss"


ALLOWED_DOMAINS = [
    "reuters.com", "bbc.com", "bbc.co.uk", "aa.com.tr", "dw.com", "tr.euronews.com", "euronews.com"
]

# JSON dosyasÄ± yoksa oluÅŸtur
if not os.path.exists(DATA_JSON):
    with open(DATA_JSON, "w", encoding="utf-8") as f:
        json.dump([], f, ensure_ascii=False, indent=2)

# ====== Lazy helpers ======
def _lazy_import(name: str):
    try:
        return importlib.import_module(name)
    except Exception:
        return None

def _lazy_bs4():
    return _lazy_import("bs4")

def _lazy_sentence_model():
    st = _lazy_import("sentence_transformers")
    if not st:
        return None
    try:
        # kÃ¼Ã§Ã¼k ve Ã§ok dilli model (ilk kullanÄ±mda yÃ¼klenir)
        return st.SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    except Exception:
        return None

def _lazy_faiss():
    return _lazy_import("faiss") or _lazy_import("faiss_cpu")

# ====== Metin temizlik ======
def _clean_text(text: str) -> str:
    if not text:
        return ""
    bs4 = _lazy_bs4()
    if bs4:
        text = bs4.BeautifulSoup(text, "html.parser").get_text(" ", strip=True)
    text = re.sub(r"#{1,6}\s*", "", text)
    text = re.sub(r"\*{1,3}", "", text)
    text = text.replace("â€¢", " ").replace("â€“", "-").replace("â€”", "-")
    text = re.sub(r"\s{2,}", " ", text).strip()
    return text

def _limit_by_sentences(text: str, sentence_count: int) -> str:
    sentences = re.split(r"(?<=[.!?])\s+", text)
    out = " ".join(sentences[:max(1, sentence_count)])
    if len(out) > 1500:
        out = out[:1500].rstrip() + "..."
    return out

# ====== Wikipedia REST summary ======
def _wiki_summary_for_term(term: str, lang: str = "tr") -> dict | None:
    url = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{quote(term)}"
    try:
        r = requests.get(url, headers=HEADERS, timeout=10)
        if r.status_code != 200:
            return None
        return r.json()
    except Exception:
        return None

def wiki_ozet_with_meta(konu: str, cumle: int = 6) -> dict | None:
    temiz = konu.lower()
    for k in ["kimdir","nedir","hayatÄ±","hayatÄ±nÄ±","biyografisi","tarihi","anlamÄ±","Ã¶zeti","Ã¶zetle","anlat","hikayesi","kim","kimdi","kimmiÅŸ","kimin","hakkÄ±nda","bilgi ver"]:
        temiz = temiz.replace(k, "")
    temiz = temiz.strip()

    for lang in ("tr", "en"):
        data = _wiki_summary_for_term(temiz, lang=lang)
        if not data or "extract" not in data:
            continue
        extract = _clean_text(data.get("extract") or "")
        if not extract:
            continue
        tp = data.get("type") or "standard"
        text = _limit_by_sentences(extract, cumle)
        cu = data.get("content_urls") or {}
        desktop = cu.get("desktop") or {}
        page_url = desktop.get("page")
        return {
            "text": text.strip(),
            "url": page_url or f"https://{lang}.wikipedia.org/",
            "title": data.get("title") or "",
            "lang": lang,
            "type": tp
        }
    return None


def wiki_ozet(konu: str, cumle: int = 6) -> str | None:
    meta = wiki_ozet_with_meta(konu, cumle=cumle)
    return meta["text"] if meta else None

# ====== JSON depolama ======
def _json_oku() -> list:
    with open(DATA_JSON, "r", encoding="utf-8") as f:
        return json.load(f)

def _json_yaz(veriler: list):
    """JSONâ€™a geri yazar."""
    with open(DATA_JSON, "w", encoding="utf-8") as f:
        json.dump(veriler, f, ensure_ascii=False, indent=2)

# ====== FAISS + Embeddings (opsiyonel) ======
_model_cache = None
def _get_model():
    global _model_cache
    if _model_cache is None:
        if SWEAX_LIGHT:
            return None  # en hafif modda hiÃ§ aÃ§ma
        _model_cache = _lazy_sentence_model()
    return _model_cache

def bilgi_kaydet(konu: str, metin: str):
    # JSON'a yaz
    veriler = _json_oku()
    veriler.append({"konu": konu, "icerik": metin})
    _json_yaz(veriler)

    # Embedding/FAISS yoksa sessizce pas geÃ§
    model = _get_model()
    faiss = _lazy_faiss()
    if not (model and faiss):
        return

    try:
        v = model.encode([metin]).astype("float32")
        if os.path.exists(FAISS_FILE):
            index = faiss.read_index(FAISS_FILE)
        else:
            index = faiss.IndexFlatL2(v.shape[1])
        index.add(v)
        faiss.write_index(index, FAISS_FILE)
    except Exception:
        pass  # RAM/baÄŸÄ±mlÄ±lÄ±k sorunlarÄ±nda sessiz geÃ§

def bilgi_bul(soru: str, top_k: int = 2) -> list[dict]:
    # FAISS yoksa sadece JSON iÃ§inden son kayÄ±tlarÄ± dÃ¶ndÃ¼rme stratejisi
    model = _get_model()
    faiss = _lazy_faiss()
    if not (model and faiss and os.path.exists(FAISS_FILE)):
        # hafif fallback: konu kelimesi geÃ§en son kayÄ±tlar
        veriler = _json_oku()
        s = soru.lower()
        return [v for v in reversed(veriler) if v["konu"].lower() in s][:top_k]

    try:
        index = faiss.read_index(FAISS_FILE)
        veriler = _json_oku()
        q = model.encode([soru]).astype("float32")
        _, idx = index.search(q, top_k)
        out = []
        for i in idx[0]:
            if 0 <= i < len(veriler):
                out.append(veriler[i])
        return out
    except Exception:
        return []

# ====== GÃ¼ncellik + Web (hafif) ======
TR_AYLAR = {"ocak":1,"ÅŸubat":2,"mart":3,"nisan":4,"mayÄ±s":5,"haziran":6,"temmuz":7,"aÄŸustos":8,"eylÃ¼l":9,"ekim":10,"kasÄ±m":11,"aralÄ±k":12}

def _bugun_ist() -> datetime:
    return datetime.now(ZoneInfo("Europe/Istanbul"))

def _metinden_tarih_bul(metin: str) -> datetime | None:
    s = metin.lower()
    m = re.search(r"(\d{1,2})\s+([a-zÃ§ÄŸÄ±Ã¶ÅŸÃ¼]+)(?:\s+(\d{4}))?", s)
    if not m: return None
    gun = int(m.group(1)); ay_ad = m.group(2); yil = int(m.group(3)) if m.group(3) else _bugun_ist().year
    ay = TR_AYLAR.get(ay_ad);
    if not ay: return None
    try:
        return datetime(yil, ay, gun, tzinfo=ZoneInfo("Europe/Istanbul"))
    except Exception:
        return None

def soru_guncel_mi(soru: str) -> bool:
    s = soru.lower()
    if any(k in s for k in ["bugÃ¼n","dÃ¼n","az Ã¶nce","son dakika","geÃ§en hafta","bu hafta"]):
        return True
    return _metinden_tarih_bul(soru) is not None

def _domain_izinli_mi(url: str) -> bool:
    return any(d in url for d in ALLOWED_DOMAINS)

def web_ara_ddg(query: str, max_results: int = 2) -> list[dict]:
    url = "https://duckduckgo.com/lite/"
    try:
        html = requests.get(url, params={"q": query}, timeout=8, headers=HEADERS).text
        bs4 = _lazy_bs4()
        if not bs4:
            return []
        soup = bs4.BeautifulSoup(html, "html.parser")
        links = []
        for a in soup.select("a"):
            href = a.get("href") or ""
            title = (a.text or "").strip()
            if href.startswith("http") and _domain_izinli_mi(href):
                links.append({"title": title[:120], "url": href})
            if len(links) >= max_results:
                break
        return links
    except Exception:
        return []

def sayfa_icerik_al(url: str, max_chars: int = 1600) -> str | None:
    try:
        r = requests.get(url, timeout=8, headers=HEADERS)
        bs4 = _lazy_bs4()
        if not bs4:
            return None
        soup = bs4.BeautifulSoup(r.text, "html.parser")
        texts = []
        for tag in soup.find_all(["h1","h2","p"]):
            t = (tag.get_text(" ", strip=True) or "")
            if t: texts.append(t)
        full = " ".join(texts)
        return full[:max_chars] if full else None
    except Exception:
        return None

# ====== Ana RAG cevabÄ± ======
def rag_cevap_uret(soru: str, model_cevap: str) -> str:
    # GÃ¼ncel ise kÄ±sa web Ã¶zetini gizlice Ã§ek â€” ama RAMâ€™e yÃ¼k bindirme
    if soru_guncel_mi(soru):
        for l in web_ara_ddg(soru, max_results=2):
            _ = sayfa_icerik_al(l["url"])  # ileride iÃ§ baÄŸlama eklenebilir
        return model_cevap

    # FAISS/JSON ile destek
    kaynaklar = bilgi_bul(soru)
    if not kaynaklar:
        yeni = wiki_ozet(soru, cumle=5)
        if yeni:
            bilgi_kaydet(soru, yeni)
    return model_cevap

# WEBDE ARAMA


def web_ara_genel(soru: str, max_results: int = 3) -> str | None:
    """
    DuckDuckGo HTML arama (GET versiyonu) â€“ Render uyumlu, daha dayanÄ±klÄ± sÃ¼rÃ¼m
    """
    import requests
    from bs4 import BeautifulSoup

    try:
        url = "https://duckduckgo.com/html/"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/118.0.5993.70 Safari/537.36"
        }

        # GET kullanÄ±yoruz Ã§Ã¼nkÃ¼ POST bazen boÅŸ dÃ¶ndÃ¼rÃ¼yor
        resp = requests.get(url, params={"q": soru}, headers=headers, timeout=10)
        html = resp.text
        soup = BeautifulSoup(html, "html.parser")

        results = []
        for a in soup.select(".result__a"):
            title = (a.text or "").strip()
            href = a.get("href")
            if href and href.startswith("http"):
                results.append(f"ğŸ”— [{title}]({href})")
            if len(results) >= max_results:
                break

        if not results:
            # Alternatif: Google fallback (proxy)
            g_url = f"https://www.google.com/search?q={soru.replace(' ', '+')}"
            return f"ğŸŒ SonuÃ§ bulunamadÄ±. Åu sayfada aramayÄ± deneyebilirsin:\nğŸ”— [Google'da Ara]({g_url})"

        return "ğŸŒ Web sonuÃ§larÄ±:\n" + "\n".join(results)

    except Exception as e:
        return f"âš ï¸ Web arama hatasÄ±: {e}"


def web_ara_serper(soru: str, max_results: int = 3) -> str | None:
    """
    Serper.dev Google Search API â€” Markdown tabanlÄ± ChatGPT tarzÄ± kart stili sonuÃ§
    """
    import requests, os
    SERPER_KEY = os.getenv("SERPER_KEY", "60616d6ea2b6da230c1930c4817a755439d44cba")

    try:
        headers = {"X-API-KEY": SERPER_KEY, "Content-Type": "application/json"}
        payload = {"q": soru, "gl": "tr", "hl": "tr"}
        r = requests.post("https://google.serper.dev/search", json=payload, headers=headers, timeout=10)
        data = r.json()

        if "organic" not in data:
            return None

        results = data["organic"][:max_results]
        if not results:
            return "ğŸ” ÃœzgÃ¼nÃ¼m, bu konuda gÃ¼ncel bilgi bulunamadÄ±."

        # BaÅŸlÄ±k
        yanit = [f"ğŸ’ **{soru.title()} (GÃ¼ncel Bilgiler)**", "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"]

        # SonuÃ§larÄ± dÃ¼zenle
        for r in results:
            baslik = r.get("title", "Kaynak").strip()
            link = r.get("link", "")
            aciklama = r.get("snippet", "").replace("\n", " ").strip()
            if len(aciklama) > 220:
                aciklama = aciklama[:220] + "â€¦"

            # Basit domain Ã§Ä±karÄ±mÄ±
            domain = ""
            if "://" in link:
                domain = link.split("/")[2].replace("www.", "")

            yanit.append(
                f"ğŸ“ **{domain}**\n"
                f"ğŸ“° [{baslik}]({link})\n"
                f"ğŸ’¬ {aciklama}\n"
                "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
            )
        # SonuÃ§larÄ± birleÅŸtirirken kÄ±sa Ã¶zet Ã¼ret
        joined = " ".join(r.get("snippet", "") for r in results if r.get("snippet"))
        if joined:
            yanit += ["\nğŸ§  **KÄ±sa Ã–zet:** " + joined[:600] + "â€¦"]
        return "\n".join(yanit)

    except Exception as e:
        return f"âš ï¸ Web arama hatasÄ±: {e}"



def _guncel_sorgu_mu(soru: str) -> bool:
    s = soru.lower()
    anahtarlar = [
        "bugÃ¼n", "ÅŸu an", "gÃ¼ncel", "fiyat", "nerede", "hangi", "trend", "popÃ¼ler",
        "oyunlar", "hava", "puan", "menÃ¼", "yeni Ã§Ä±kan", "en iyi", "yakÄ±n", "restoran"
    ]
    return any(k in s for k in anahtarlar)

def web_fallback_ara(soru: str) -> str:

    sonuc = web_ara_serper(soru)
    if sonuc:
        return sonuc
    sonuc = web_ara_genel(soru)
    if sonuc:
        return sonuc
    return "ğŸŒ Bu konuda gÃ¼venilir bir kaynak bulunamadÄ±."